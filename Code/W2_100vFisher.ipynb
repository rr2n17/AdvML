{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\n",
      "Libraries are imported.\n",
      "FastText commentaries started to fit .\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from gensim.models.fasttext import FastText\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(\"==================================\")\n",
    "print(\"Libraries are imported.\")\n",
    "print(\"FastText commentaries started to fit .\")\n",
    "\n",
    "\n",
    "def get_directory_content(path, extension):\n",
    "    \"\"\" Returns directory content of a particular extension \"\"\"\n",
    "    matches = []\n",
    "    for root, dirnames, filenames in os.walk(path):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(('.{}'.format(extension))):\n",
    "                matches.append(os.path.join(root, filename))\n",
    "    return matches\n",
    "\n",
    "\n",
    "def load_file(filename):\n",
    "    \"\"\" Loads the files with comments.\n",
    "        Returns label and the dataset. \"\"\"\n",
    "    label = filename.split('/')[-1][:-12]\n",
    "    return label, pd.read_json(filename)\n",
    "\n",
    "\n",
    "def word_to_vec_to_fisher(sentence, model, gmm):\n",
    "    \"\"\" Transforms list of words of a comment into a \"\"\"\n",
    "    storage = []\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            if word:\n",
    "                word = re.sub('r[^a-zA-Z]+', \"\", str(word)).lower()\n",
    "            else:\n",
    "                continue\n",
    "            model[word].any()\n",
    "            storage.append(model[word])\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    if not storage:\n",
    "        print(sentence, storage)\n",
    "        return -999 # changed here\n",
    "        \n",
    "    xx = np.atleast_2d(storage)\n",
    "    N = xx.shape[0]\n",
    "\n",
    "    # Compute posterior probabilities.\n",
    "    Q = gmm.predict_proba(xx)  # NxK\n",
    "\n",
    "    # Compute the sufficient statistics of descriptors.\n",
    "    Q_sum = np.sum(Q, 0)[:, np.newaxis] / N\n",
    "    Q_xx = np.dot(Q.T, xx) / N\n",
    "    Q_xx_2 = np.dot(Q.T, xx ** 2) / N\n",
    "\n",
    "    # Compute derivatives with respect to\n",
    "    # mixing weights, means and variances.\n",
    "    d_pi = Q_sum.squeeze() - gmm.weights_\n",
    "    d_mu = Q_xx - Q_sum * gmm.means_\n",
    "    d_sigma = ( - Q_xx_2 - Q_sum * gmm.means_ ** 2 + Q_sum * gmm.covariances_ + 2 * Q_xx * gmm.means_)\n",
    "\n",
    "    # Merge derivatives into a\n",
    "    # vector.\n",
    "    return np.hstack((d_pi, d_mu.flatten(), d_sigma.flatten()))\n",
    "\n",
    "\n",
    "def clean(lst):\n",
    "    if lst:\n",
    "        return lst\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'Insult'\n",
    "# scalernames = [\"SS\", \"MM\"]\n",
    "# stime = time.time()\n",
    "# model = FastText.load(\"Fasttext100/FastText100.bin\")\n",
    "# print(\"Model Loaded.\")\n",
    "# path = os.getcwd() + '/balanced/'\n",
    "# filenames = get_directory_content(path, 'json')\n",
    "# for idx, f in enumerate(filenames):\n",
    "#     # Fast Text (word2vec)\n",
    "#     label = f.split('/')[-1][:-12]\n",
    "#     print(filename, f)\n",
    "#     if filename not in f:\n",
    "#         continue\n",
    "#     fstime = time.time()\n",
    "#     label, df = load_file(f)\n",
    "\n",
    "#     print(label, 'has columns: ', df.columns)\n",
    "#     print(\"File {label} loaded.\".format(label=label))\n",
    "#     # Fisher's Vector\n",
    "#     # for K in [32, 64]:\n",
    "#     for K in [32]:#, 64]:\n",
    "#         gmm = GaussianMixture(n_components=K, covariance_type='diag', reg_covar=1e-4,verbose=1)\n",
    "#         gmm.fit(model.wv.vectors)\n",
    "#         target = df[df['comment_text'].map(len) > 0].iloc[:, -1] # removed empty rows\n",
    "#         # word to vect - > to fisher\n",
    "#         comments = df['comment_text'][df['comment_text'].map(len) > 0].apply(\n",
    "#             lambda lst: word_to_vec_to_fisher(lst, model, gmm))   # remove empty rows\n",
    "#         # now remove -999 !!!!\n",
    "#         # and remove the same rows from target\n",
    "#         indeces = comments[comments.apply(type) != np.ndarray].index\n",
    "#         target = target.drop(indeces)\n",
    "#         comments = comments.drop(indeces)\n",
    "#         print('Length target:', target.shape[0])\n",
    "#         print('Length comments:', comments.shape[0])\n",
    "#         comments = np.stack(comments)\n",
    "#         for idx, scaler in enumerate([StandardScaler, MinMaxScaler]):\n",
    "#             # normalise\n",
    "#             scaled = pd.DataFrame(scaler().fit_transform(comments))\n",
    "#             pca = PCA(n_components=1000)\n",
    "#             pca_data = pd.DataFrame(pca.fit_transform(scaled))\n",
    "#             # save the file\n",
    "#             pd.concat([pca_data.reset_index(drop=True), target.reset_index(drop=True)],\n",
    "#                       axis=1).to_csv(\"FT100FV{K}/{filename}{K}{scaler}ft100.csv\".format(\n",
    "#                           filename=filename,\n",
    "#                           K=K,\n",
    "#                           scaler=scalernames[idx]))\n",
    "#         print(\"{label} K={K} Scaler={scaler} csv file saved. Time:{time}\".format(\n",
    "#                   label=label, K=K, scaler=scalernames[idx], time=(time.time()-fstime)))\n",
    "#         print(\"Time spent: {time}\".format(time=(time.time()-stime)))\n",
    "#         print(\"All files saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded.\n",
      "Toxic /home/ao2u17/Desktop/FinalProject/Anton/balanced/ThreatOptimal.json\n",
      "Toxic /home/ao2u17/Desktop/FinalProject/Anton/balanced/InsultOptimal.json\n",
      "Toxic /home/ao2u17/Desktop/FinalProject/Anton/balanced/IdentityOptimal.json\n",
      "Toxic /home/ao2u17/Desktop/FinalProject/Anton/balanced/ObsceneOptimal.json\n",
      "Toxic /home/ao2u17/Desktop/FinalProject/Anton/balanced/ToxicOptimal.json\n",
      "Toxic has columns:  Index(['comment_text', 'toxic'], dtype='object')\n",
      "File Toxic loaded.\n",
      "Initialization 0\n",
      "  Iteration 0\n",
      "  Iteration 10\n",
      "  Iteration 20\n",
      "  Iteration 30\n",
      "  Iteration 40\n",
      "  Iteration 50\n",
      "Initialization converged: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ao2u17/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:44: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/home/ao2u17/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:45: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['194', '249'] []\n",
      "['160'] []\n",
      "['8235'] []\n",
      "['160'] []\n",
      "['122'] []\n",
      "Length target: 183473\n",
      "Length comments: 183473\n",
      "Before stacking:  False\n",
      "Before normaliztion:  0\n",
      "After normaliztion False\n",
      "Toxic /home/ao2u17/Desktop/FinalProject/Anton/balanced/SevereOptimal.json\n"
     ]
    }
   ],
   "source": [
    "# settings\n",
    "filename = 'Toxic'\n",
    "fasttextmodelpath = \"Fasttext100/FastText100.bin\"\n",
    "K = 32\n",
    "scaleri = 0\n",
    "\n",
    "# code\n",
    "scalers = [StandardScaler, MinMaxScaler]\n",
    "scaler = scalers[scaleri]\n",
    "scalernames = [\"SS\", \"MM\"]\n",
    "stime = time.time()\n",
    "model = FastText.load(fasttextmodelpath)\n",
    "print(\"Model Loaded.\")\n",
    "path = os.getcwd() + '/balanced/'\n",
    "filenames = get_directory_content(path, 'json')\n",
    "for idx, f in enumerate(filenames):\n",
    "    # Fast Text (word2vec)\n",
    "    label = f.split('/')[-1][:-12]\n",
    "    print(filename, f)\n",
    "    if filename not in f:\n",
    "        continue\n",
    "    fstime = time.time()\n",
    "    label, df = load_file(f)\n",
    "\n",
    "    print(label, 'has columns: ', df.columns)\n",
    "    print(\"File {label} loaded.\".format(label=label))\n",
    "    # Fisher's Vector\n",
    "    gmm = GaussianMixture(n_components=K, covariance_type='diag', reg_covar=1e-4,verbose=1)\n",
    "    gmm.fit(model.wv.vectors)\n",
    "    target = df[df['comment_text'].map(len) > 0].iloc[:, -1] # removed empty rows\n",
    "    # word to vect - > to fisher\n",
    "    comments = df['comment_text'][df['comment_text'].map(len) > 0].apply(\n",
    "        lambda lst: word_to_vec_to_fisher(lst, model, gmm))   # remove empty rows\n",
    "    # now remove -999 !!!!\n",
    "    # and remove the same rows from target\n",
    "    indices = comments[comments.apply(type) != np.ndarray].index\n",
    "    del df\n",
    "    target = target.drop(indices)\n",
    "    comments = comments.drop(indices)\n",
    "    print('Length target:', target.shape[0])\n",
    "    print('Length comments:', comments.shape[0])\n",
    "    print(\"Before stacking: \", comments.isnull().values.any())\n",
    "    comments = np.stack(comments).astype('float16')\n",
    "    # normalise\n",
    "    print(\"Before normaliztion: \", np.isnan(comments).sum())\n",
    "    comments = pd.DataFrame(scaler().fit_transform(comments)).fillna(0)\n",
    "    print(\"After normaliztion\", comments.isnull().values.any())\n",
    "    pca = PCA(n_components=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After normaliztion False\n",
      "Severe K=32 Scaler=SS csv file saved. Time:1159.9676609039307\n",
      "Time spent: 1170.92329454422\n",
      "All files saved.\n"
     ]
    }
   ],
   "source": [
    "print(\"After normaliztion\", comments.isnull().values.any())\n",
    "comments = comments.replace(np.inf, 0) # replaces inf with 0\n",
    "comments = pd.DataFrame(pca.fit_transform(comments))\n",
    "# save the file\n",
    "pd.concat([comments.reset_index(drop=True), target.reset_index(drop=True)],\n",
    "          axis=1).to_csv(\"FT100FV{K}/{filename}{K}{scaler}ft100.csv\".format(\n",
    "              filename=filename,\n",
    "              K=K,\n",
    "              scaler=scalernames[scaleri]))\n",
    "print(\"{label} K={K} Scaler={scaler} csv file saved. Time:{time}\".format(\n",
    "          label=label, K=K, scaler=scalernames[scaleri], time=(time.time()-fstime)))\n",
    "print(\"Time spent: {time}\".format(time=(time.time()-stime)))\n",
    "print(\"All files saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for iix, iin in enumerate(comments.max(axis=0)):\n",
    "#     if iin == np.inf:\n",
    "#         print(iix, iin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('comments', 1467784104),\n",
       " ('target', 2935592),\n",
       " ('GaussianMixture', 2000),\n",
       " ('PCA', 1464),\n",
       " ('MinMaxScaler', 1184),\n",
       " ('scaler', 1184),\n",
       " ('FastText', 1056),\n",
       " ('StandardScaler', 1056),\n",
       " ('clean', 136),\n",
       " ('get_directory_content', 136),\n",
       " ('load_file', 136),\n",
       " ('word_to_vec_to_fisher', 136),\n",
       " ('filenames', 128),\n",
       " ('f', 116),\n",
       " ('path', 98),\n",
       " ('np', 80),\n",
       " ('pd', 80),\n",
       " ('scalernames', 80),\n",
       " ('scalers', 80),\n",
       " ('fasttextmodelpath', 76),\n",
       " ('indices', 64),\n",
       " ('gmm', 56),\n",
       " ('model', 56),\n",
       " ('pca', 56),\n",
       " ('label', 55),\n",
       " ('filename', 54),\n",
       " ('K', 28),\n",
       " ('idx', 28),\n",
       " ('scaleri', 28),\n",
       " ('fstime', 24),\n",
       " ('stime', 24)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# These are the usual ipython objects, including this one you are creating\n",
    "ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "# Get a sorted list of the objects and their sizes\n",
    "sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.467784104"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1467784104 / 1000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
