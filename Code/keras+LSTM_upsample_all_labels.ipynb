{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM with Upsampled Data and single label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rr2n17/.conda/envs/myenv/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras import losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_csv('train.csv')\n",
    "# test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity = pd.read_json('IdentityOptimal.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[agreed, addition, reds, give, angular, amplit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Fischer, comments, attacks, added, quotations...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[hey, Chris, wast, windpipe, six, ass, deleted...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>[authoritarian, little, hitless]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>[editor, thermoformed, mod, administrator, win...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           comment_text  identity_hate\n",
       "0     [agreed, addition, reds, give, angular, amplit...              0\n",
       "1     [Fischer, comments, attacks, added, quotations...              0\n",
       "10    [hey, Chris, wast, windpipe, six, ass, deleted...              0\n",
       "100                    [authoritarian, little, hitless]              0\n",
       "1000  [editor, thermoformed, mod, administrator, win...              0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "comment_text     False\n",
       "identity_hate    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To check for NULL values in training and test data\n",
    "identity.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the dependent variables are in the training set itself so we need to split them up, into X and Y sets.\n",
    "#list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "list_classes = [\"identity_hate\"]\n",
    "y = identity[list_classes].values\n",
    "list_sentences_train = identity[\"comment_text\"]\n",
    "#list_sentences_test = test[\"comment_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to feed the comments into the LSTM as part of the neural network, these steps should be followed:\n",
    "# 1) Tokenization - We need to break down the sentence into unique words. For eg, \"I love cats and love dogs\" will become [\"I\",\"love\",\"cats\",\"and\",\"dogs\"]\n",
    "# 2) Indexing - We put the words in a dictionary-like structure and give them an index each For eg, {1:\"I\",2:\"love\",3:\"cats\",4:\"and\",5:\"dogs\"}\n",
    "# 3) Index Representation- We could represent the sequence of words in the comments in the form of index, and feed this chain of index into our LSTM. For eg, [1,2,3,4,2,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "tokenizer = Tokenizer(num_words = max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.preprocessing.text.Tokenizer at 0x7f7a9a3cf7f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(list(list_sentences_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "#list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2054,\n",
       "  1499,\n",
       "  4735,\n",
       "  100,\n",
       "  14111,\n",
       "  11829,\n",
       "  3795,\n",
       "  1403,\n",
       "  418,\n",
       "  18937,\n",
       "  84,\n",
       "  84,\n",
       "  188,\n",
       "  84,\n",
       "  125,\n",
       "  931,\n",
       "  3646]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_tokenized_train[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have to feed a stream of data that has a consistent length(fixed number of features). Use padding for this.\n",
    "#trim the longer sentences to the same length(maxlen) as the short ones. In this case, we have set the max length to be 200.\n",
    "maxlen = 200\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "#X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to see distribution of words\n",
    "totalNumWords = [len(one_comment) for one_comment in list_tokenized_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(totalNumWords,bins = np.arange(0,410,10))#[0,50,100,150,200,250,300,350,400])#,450,500,550,600,650,700,750,800,850,900])\n",
    "plt.show()\n",
    "#output shows that most of the sentence length is about 30+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the inputs into our networks are our list of encoded sentences.\n",
    "#We begin defining an Input layer that accepts a list of sentences that has a dimension of 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen, )) #maxlen=200 as defined earlier\n",
    "#By indicating an empty space after comma, we are telling Keras to infer the number automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we project the words to a defined vector space depending on the distance of the surrounding words in a sentence\n",
    "embed_size = 128\n",
    "x = Embedding(max_features, embed_size)(inp)\n",
    "#The output of the Embedding layer is just a list of the coordinates of the words in this vector space.\n",
    "#For eg. (-81.012) for \"cat\" and (-80.012) for \"dog\".\n",
    "#We could also use the distance of these coordinates to detect relevance and context. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We set the LSTM to produce an output that has a dimension of 60 and want it to return the whole unrolled sequence of results.\n",
    "x = LSTM(60, return_sequences=True,name='lstm_layer')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the output is a tensor.\n",
    "#To reshape 3D tensor to 2D, we use a Global Max Pooling layer which is traditionally used in CNN problems\n",
    "#to reduce the dimensionality of image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = GlobalMaxPool1D()(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Dropout(0.1)(x)\n",
    "#After a drop out layer, we connect the output of drop out layer to a densely connected layer\n",
    "#and the output passes through a RELU function.\n",
    "\n",
    "#Activation( (Input X Weights) + Bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Dense(50, activation=\"relu\")(x) #define the Dense layer to produce an output dimension of 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feed output into dropout layer again\n",
    "x = Dropout(0.1)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feed output to Sigmoid Layer\n",
    "x = Dense(1, activation=\"sigmoid\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have set our model to optimize our loss function using Adam optimizer, \n",
    "# define the loss function to be \"binary_crossentropy\" since we are tackling a binary classification.\n",
    "# Default learning rate is set at 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=inp, outputs=x)\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#                   optimizer='adam',\n",
    "#                   metrics=['accuracy'])\n",
    "model.compile(loss='mean_squared_error',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13488 samples, validate on 3372 samples\n",
      "Epoch 1/2\n",
      "13488/13488 [==============================] - 110s 8ms/step - loss: 0.0828 - acc: 0.8928 - val_loss: 0.0235 - val_acc: 0.9674\n",
      "Epoch 2/2\n",
      "13488/13488 [==============================] - 113s 8ms/step - loss: 0.0189 - acc: 0.9766 - val_loss: 0.0237 - val_acc: 0.9680\n"
     ]
    }
   ],
   "source": [
    "batch_size = 30\n",
    "epochs = 2\n",
    "model_result = model.fit(X_t,y, batch_size=batch_size, epochs=epochs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, input_dim=60))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " ...\n",
      " [1.0000000e+00]\n",
      " [9.9999988e-01]\n",
      " [1.0466949e-18]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_proba(X_t)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17383676450243005"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(X_t)\n",
    "# len(y_pred)\n",
    "difference = y_pred-y\n",
    "#len(difference)\n",
    "avg = np.average(difference)\n",
    "avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('lstm_model_toxic.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "# # load model from single file\n",
    "# model = load_model('lstm_model_toxic.h5')\n",
    "# # make predictions\n",
    "# yhat = model.predict(X_t, verbose=0)\n",
    "# print(yhat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
