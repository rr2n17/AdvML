{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM with Upsampled Data and single label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9202345109833095951\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 179992166\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 18127857902157479760\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 980, pci bus id: 0000:04:00.0, compute capability: 5.2\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras import losses\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install np_utils\n",
    "# !pip uninstall tensorflow\n",
    "# !pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_csv('train.csv')\n",
    "# test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity = pd.read_json('../Datasets/balanced_one_label/ToxicOptimal.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity = identity.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(183528, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>164487</th>\n",
       "      <td>[Hey, clown, meat, doll, fucks, dog, Goethe, p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146224</th>\n",
       "      <td>[stop, working, stupid, ass, bitch, ghosts, pa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121469</th>\n",
       "      <td>[welcome, discussion, page, communist, asshole...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96660</th>\n",
       "      <td>[User, Talk, Hesperus, asked, someone, opinion...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175003</th>\n",
       "      <td>[You, toilet, dally, indeterminate, blockade, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_text  toxic\n",
       "164487  [Hey, clown, meat, doll, fucks, dog, Goethe, p...      1\n",
       "146224  [stop, working, stupid, ass, bitch, ghosts, pa...      1\n",
       "121469  [welcome, discussion, page, communist, asshole...      1\n",
       "96660   [User, Talk, Hesperus, asked, someone, opinion...      1\n",
       "175003  [You, toilet, dally, indeterminate, blockade, ...      1"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(identity.shape)\n",
    "identity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "comment_text    False\n",
       "toxic           False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To check for NULL values in training and test data\n",
    "identity.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the dependent variables are in the training set itself so we need to split them up, into X and Y sets.\n",
    "#list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "list_classes = [\"toxic\"]\n",
    "y = identity[list_classes].values\n",
    "list_sentences_train = identity[\"comment_text\"]\n",
    "#list_sentences_test = test[\"comment_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_sentences_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to feed the comments into the LSTM as part of the neural network, these steps should be followed:\n",
    "# 1) Tokenization - We need to break down the sentence into unique words. For eg, \"I love cats and love dogs\" will become [\"I\",\"love\",\"cats\",\"and\",\"dogs\"]\n",
    "# 2) Indexing - We put the words in a dictionary-like structure and give them an index each For eg, {1:\"I\",2:\"love\",3:\"cats\",4:\"and\",5:\"dogs\"}\n",
    "# 3) Index Representation- We could represent the sequence of words in the comments in the form of index, and feed this chain of index into our LSTM. For eg, [1,2,3,4,2,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "tokenizer = Tokenizer(num_words = max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.preprocessing.text.Tokenizer at 0x1beb04e1f60>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(list(list_sentences_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "#list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[457, 2177, 2000, 5960, 1862, 416, 8352, 116, 123, 18, 7598, 427, 178]]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_tokenized_train[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have to feed a stream of data that has a consistent length(fixed number of features). Use padding for this.\n",
    "#trim the longer sentences to the same length(maxlen) as the short ones. In this case, we have set the max length to be 200.\n",
    "maxlen = 200\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "#X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to see distribution of words\n",
    "totalNumWords = [len(one_comment) for one_comment in list_tokenized_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(totalNumWords,bins = np.arange(0,410,10))#[0,50,100,150,200,250,300,350,400])#,450,500,550,600,650,700,750,800,850,900])\n",
    "# plt.show()\n",
    "#output shows that most of the sentence length is about 30+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the inputs into our networks are our list of encoded sentences.\n",
    "#We begin defining an Input layer that accepts a list of sentences that has a dimension of 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen, )) #maxlen=200 as defined earlier\n",
    "#By indicating an empty space after comma, we are telling Keras to infer the number automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we project the words to a defined vector space depending on the distance of the surrounding words in a sentence\n",
    "embed_size = 128\n",
    "x = Embedding(max_features, embed_size)(inp)\n",
    "#The output of the Embedding layer is just a list of the coordinates of the words in this vector space.\n",
    "#For eg. (-81.012) for \"cat\" and (-80.012) for \"dog\".\n",
    "#We could also use the distance of these coordinates to detect relevance and context. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### LSTM Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We set the LSTM to produce an output that has a dimension of 60 and want it to return the whole unrolled sequence of results.\n",
    "x = LSTM(60, return_sequences=True,name='lstm_layer')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the output is a tensor.\n",
    "#To reshape 3D tensor to 2D, we use a Global Max Pooling layer which is traditionally used in CNN problems\n",
    "#to reduce the dimensionality of image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = GlobalMaxPool1D()(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Dropout(0.1)(x)\n",
    "#After a drop out layer, we connect the output of drop out layer to a densely connected layer\n",
    "#and the output passes through a RELU function.\n",
    "\n",
    "#Activation( (Input X Weights) + Bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = Dense(50, activation=\"relu\")(x) #define the Dense layer to produce an output dimension of 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feed output into dropout layer again\n",
    "x = Dropout(0.1)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feed output to Sigmoid Layer\n",
    "x = Dense(1, activation=\"sigmoid\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have set our model to optimize our loss function using Adam optimizer, \n",
    "# define the loss function to be \"binary_crossentropy\" since we are tackling a binary classification.\n",
    "# Default learning rate is set at 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=inp, outputs=x)\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#                   optimizer='adam',\n",
    "#                   metrics=['accuracy'])\n",
    "model.compile(loss='mean_squared_error',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 68000 samples, validate on 17000 samples\n",
      "Epoch 1/3\n",
      "68000/68000 [==============================] - 141s 2ms/step - loss: 0.0498 - acc: 0.9324 - val_loss: 0.0220 - val_acc: 0.9721\n",
      "Epoch 2/3\n",
      "68000/68000 [==============================] - 139s 2ms/step - loss: 0.0153 - acc: 0.9812 - val_loss: 0.0200 - val_acc: 0.9743\n",
      "Epoch 3/3\n",
      "68000/68000 [==============================] - 139s 2ms/step - loss: 0.0105 - acc: 0.9876 - val_loss: 0.0205 - val_acc: 0.9740\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 3\n",
    "model_result = model.fit(X_t[0:183528],y[0:85000], batch_size=batch_size, epochs=epochs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating predictions on  the remaining dataset\n",
    "predictions = model.predict(X_t[85000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(y)\n",
    "tmp = pd.DataFrame(columns = {\"true\"})\n",
    "tmp[\"true\"]  = df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.DataFrame(predictions)\n",
    "t[t[0] > 0.5] = 1\n",
    "t[t[0] < 0.5] = 0\n",
    "t = pd.concat([t, tmp.reset_index(drop = True)], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[\"difference\"] = t[0] - t[\"true\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005879882402351953"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = t[\"difference\"].sum()/ len(predictions)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Dense(50, input_dim=200))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dense(1, input_dim=60))\n",
    "# model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " ...\n",
      " [1.0000000e+00]\n",
      " [9.9999988e-01]\n",
      " [1.0466949e-18]]\n"
     ]
    }
   ],
   "source": [
    "# y_pred = model.predict_proba(X_t)\n",
    "# print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17383676450243005"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # len(X_t)\n",
    "# # len(y_pred)\n",
    "# difference = y_pred-y\n",
    "# #len(difference)\n",
    "# avg = np.average(difference)\n",
    "# avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('lstm_model_insult.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "# # load model from single file\n",
    "# model = load_model('lstm_model_toxic.h5')\n",
    "# # make predictions\n",
    "# yhat = model.predict(X_t, verbose=0)\n",
    "# print(yhat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
